{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gokul.kumar/Desktop/mma/preprocessing/OFA\n"
     ]
    }
   ],
   "source": [
    "%cd OFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t\t   colab.md\texamples\t  run_scripts  utils\n",
      "README.md\t\t   criterions\tfairseq\t\t  spaces.md\n",
      "README_EncouragingLoss.md  data\t\tmodels\t\t  tasks\n",
      "checkpoints\t\t   datasets.md\tofa_module\t  train.py\n",
      "checkpoints.md\t\t   evaluate.py\trequirements.txt  trainer.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fairseq import utils,tasks\n",
    "from fairseq import checkpoint_utils\n",
    "from utils.eval_utils import eval_step\n",
    "from tasks.mm_tasks.caption import CaptionTask\n",
    "from models.ofa import OFAModel\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Register caption task\n",
    "tasks.register_task('caption',CaptionTask)\n",
    "\n",
    "# turn on cuda if GPU is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use fp16 only when GPU is available\n",
    "use_fp16 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-15 23:58:08 | INFO | tasks.ofa_task | source dictionary: 59457 types\n",
      "2022-04-15 23:58:08 | INFO | tasks.ofa_task | target dictionary: 59457 types\n",
      "/home/gokul.kumar/.conda/envs/memes/lib/python3.9/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "overrides={\"bpe_dir\":\"utils/BPE\", \"eval_cider\":False, \"beam\":5, \"max_len_b\":16, \"no_repeat_ngram_size\":3, \"seed\":7}\n",
    "models, cfg, task = checkpoint_utils.load_model_ensemble_and_task(\n",
    "        utils.split_paths('checkpoints/caption.pt'),\n",
    "        arg_overrides=overrides\n",
    "    )\n",
    "\n",
    "# Move models to GPU\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    if use_fp16:\n",
    "        model.half()\n",
    "    if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n",
    "        model.cuda()\n",
    "    model.prepare_for_inference_(cfg)\n",
    "\n",
    "# Initialize generator\n",
    "generator = task.build_generator(models, cfg.generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3993005/3796615175.py:8: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms.Resize((cfg.task.patch_image_size, cfg.task.patch_image_size), interpolation=Image.BICUBIC),\n",
      "/home/gokul.kumar/.conda/envs/memes/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Image transform\n",
    "from torchvision import transforms\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "patch_resize_transform = transforms.Compose([\n",
    "    lambda image: image.convert(\"RGB\"),\n",
    "    transforms.Resize((cfg.task.patch_image_size, cfg.task.patch_image_size), interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# Text preprocess\n",
    "bos_item = torch.LongTensor([task.src_dict.bos()])\n",
    "eos_item = torch.LongTensor([task.src_dict.eos()])\n",
    "pad_idx = task.src_dict.pad()\n",
    "def encode_text(text, length=None, append_bos=False, append_eos=False):\n",
    "    s = task.tgt_dict.encode_line(\n",
    "        line=task.bpe.encode(text),\n",
    "        add_if_not_exist=False,\n",
    "        append_eos=False\n",
    "    ).long()\n",
    "    if length is not None:\n",
    "        s = s[:length]\n",
    "    if append_bos:\n",
    "        s = torch.cat([bos_item, s])\n",
    "    if append_eos:\n",
    "        s = torch.cat([s, eos_item])\n",
    "    return s\n",
    "\n",
    "# Construct input for caption task\n",
    "def construct_sample(image: Image):\n",
    "    patch_image = patch_resize_transform(image).unsqueeze(0)\n",
    "    patch_mask = torch.tensor([True])\n",
    "    src_text = encode_text(\" what does the image describe?\", append_bos=True, append_eos=True).unsqueeze(0)\n",
    "    src_length = torch.LongTensor([s.ne(pad_idx).long().sum() for s in src_text])\n",
    "    sample = {\n",
    "        \"id\":np.array(['42']),\n",
    "        \"net_input\": {\n",
    "            \"src_tokens\": src_text,\n",
    "            \"src_lengths\": src_length,\n",
    "            \"patch_images\": patch_image,\n",
    "            \"patch_masks\": patch_mask\n",
    "        }\n",
    "    }\n",
    "    return sample\n",
    "  \n",
    "# Function to turn FP32 to FP16\n",
    "def apply_half(t):\n",
    "    if t.dtype is torch.float32:\n",
    "        return t.to(dtype=torch.half)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "      <th>text_idx</th>\n",
       "      <th>pseudo_text_idx</th>\n",
       "      <th>pseudo_img_idx</th>\n",
       "      <th>gold_pc</th>\n",
       "      <th>gold_attack</th>\n",
       "      <th>...</th>\n",
       "      <th>religion_pc</th>\n",
       "      <th>sex_pc</th>\n",
       "      <th>attack_empty_attack</th>\n",
       "      <th>contempt_attack</th>\n",
       "      <th>dehumanizing_attack</th>\n",
       "      <th>exclusion_attack</th>\n",
       "      <th>inciting_violence_attack</th>\n",
       "      <th>inferiority_attack</th>\n",
       "      <th>mocking_attack</th>\n",
       "      <th>slurs_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>4901</td>\n",
       "      <td>0</td>\n",
       "      <td>['pc_empty']</td>\n",
       "      <td>['attack_empty']</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>3620</td>\n",
       "      <td>3486</td>\n",
       "      <td>['pc_empty']</td>\n",
       "      <td>['attack_empty']</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3956</td>\n",
       "      <td>['pc_empty']</td>\n",
       "      <td>['attack_empty']</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>4451</td>\n",
       "      <td>4313</td>\n",
       "      <td>['pc_empty']</td>\n",
       "      <td>['attack_empty']</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>5609</td>\n",
       "      <td>1</td>\n",
       "      <td>['pc_empty']</td>\n",
       "      <td>['attack_empty']</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  split  text_idx  \\\n",
       "0   its their character not their color that matters  train         0   \n",
       "1  don't be afraid to love again everyone is not ...  train         1   \n",
       "2                           putting bows on your pet  train         2   \n",
       "3  i love everything and everybody! except for sq...  train         3   \n",
       "4  everybody loves chocolate chip cookies, even h...  train         4   \n",
       "\n",
       "   pseudo_text_idx  pseudo_img_idx       gold_pc       gold_attack  ...  \\\n",
       "0             4901               0  ['pc_empty']  ['attack_empty']  ...   \n",
       "1             3620            3486  ['pc_empty']  ['attack_empty']  ...   \n",
       "2                0            3956  ['pc_empty']  ['attack_empty']  ...   \n",
       "3             4451            4313  ['pc_empty']  ['attack_empty']  ...   \n",
       "4             5609               1  ['pc_empty']  ['attack_empty']  ...   \n",
       "\n",
       "  religion_pc sex_pc  attack_empty_attack  contempt_attack  \\\n",
       "0         0.0    0.0                  1.0              0.0   \n",
       "1         0.0    0.0                  1.0              0.0   \n",
       "2         0.0    0.0                  1.0              0.0   \n",
       "3         0.0    0.0                  1.0              0.0   \n",
       "4         0.0    0.0                  1.0              0.0   \n",
       "\n",
       "   dehumanizing_attack  exclusion_attack  inciting_violence_attack  \\\n",
       "0                  0.0               0.0                       0.0   \n",
       "1                  0.0               0.0                       0.0   \n",
       "2                  0.0               0.0                       0.0   \n",
       "3                  0.0               0.0                       0.0   \n",
       "4                  0.0               0.0                       0.0   \n",
       "\n",
       "   inferiority_attack  mocking_attack  slurs_attack  \n",
       "0                 0.0             0.0           0.0  \n",
       "1                 0.0             0.0           0.0  \n",
       "2                 0.0             0.0           0.0  \n",
       "3                 0.0             0.0           0.0  \n",
       "4                 0.0             0.0           0.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_fp = \"../../data/hateful_memes/info_fine_grained.csv\"\n",
    "info_df = pd.read_csv(info_fp)\n",
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/gokul.kumar/Desktop/mma/preprocessing/OFA/models/sequence_generator.py:698: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  unfin_idx = bbsz_idx // beam_size\n",
      "100%|██████████| 10000/10000 [2:58:47<00:00,  1.07s/it] \n"
     ]
    }
   ],
   "source": [
    "img_folder = \"../../data/hateful_memes_masked\"\n",
    "\n",
    "captions = []\n",
    "for img_fn in tqdm(info_df['img'].str.split('/').str[1]):\n",
    "    img_fp = os.path.join(img_folder, img_fn)\n",
    "\n",
    "    img = Image.open(img_fp)\n",
    "\n",
    "    # Construct input sample & preprocess for GPU if cuda available\n",
    "    sample = construct_sample(img)\n",
    "    sample = utils.move_to_cuda(sample) if use_cuda else sample\n",
    "    sample = utils.apply_to_sample(apply_half, sample) if use_fp16 else sample\n",
    "\n",
    "    # Run eval step for caption\n",
    "    with torch.no_grad():\n",
    "        result, scores = eval_step(task, generator, models, sample)\n",
    "\n",
    "    captions.append(result[0]['caption'])\n",
    "\n",
    "    # plt.imshow(img)\n",
    "    # plt.title(result[0]['caption'])\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df['caption'] = captions\n",
    "float_cols = info_df.select_dtypes(float).columns\n",
    "info_df[float_cols] = info_df.select_dtypes(float).astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.to_csv(\"../../data/hateful_memes/hateful_memes_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2612deeb3c27f704a9fa62fbd1cc26a55750e1d3b39ee9e5544cfcd860e377fe"
  },
  "kernelspec": {
   "display_name": "mma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
